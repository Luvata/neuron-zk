---
tags: [course, stanford]
---
# CS224N - Natural language processing with Deep Learning

- Instructors: 
	- [[Christopher manning]]
	- [[Abigail See]]
	- [[Richard Socher]]

> I completed the second version of this course (2019) last August. Even though this version covers state-of-the-art Deep Learning architecture in NLP, but I found out that the lecture videos of the previous CS224N version is a little better, some early topics that [[Richard Socher]] explained are really informative, so I suggests to take a look with the previous version too.

## 1. Topic covers
### 1.1 Deep learning architectures
- Basic understanding of [[DeepLearning]], and syntax of [[Pytorch]]
- Beside basic architecture in DL, this course also cover some architectures that are extremely common in NLP:
	- [[RNN family]] in Language Model
	- [[Convolutional Neural Network]], and the application of 1D-Conv for represent out-of-vocab words
	- [[Seq2Seq Architecture]]
	- [[Attention Mechanism]] and [[Transformet Architecture]]
	
### 1.2 NLP Tasks
- Some specific NLP Tasks:
	- [[Word vector representation]]:
    	- Static: [[Word2Vec]]
    	- Contextualized word embedding: BERT, 
	- [[Subword representation]]
	- [[Language Model]]
	- [[Dependency Parsing]]
	- [[Machine Translation]]
	- [[Question Answering]]
	- [[Constituency Parsing]]
	- [[Coreference Resolution]]
	- [[Language Generation]]
	
### 1.3 Problems in NLP
- This course also addresses some problems in general of DL and NLP:
	- [[Fairness and biases]]
	- [[Low resources language translations]]

## 2. Interesting readings
